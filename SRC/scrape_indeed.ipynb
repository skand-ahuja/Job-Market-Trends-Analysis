{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Scrapping Indeed Job Listings in Jupyter Notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step-1: Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step-2: Set Up the Indeed URL for Scraping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL for Indeed job search (Data Science jobs in India)\n",
    "base_url = \"https://in.indeed.com/jobs?q=Data+Science&l=India\"\n",
    "\n",
    "# Headers to mimic a real user request\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step-3: Function to Scrape Multiple Pages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_indeed_jobs(pages=1):\n",
    "    job_list = []\n",
    "\n",
    "    for page in range(0, pages * 10, 10):  # Indeed paginates every 10 results\n",
    "        url = f\"{base_url}&start={page}\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        jobs = soup.find_all(\"div\", class_=\"job_seen_beacon\")\n",
    "\n",
    "        for job in jobs:\n",
    "            title = job.find(\"h2\").text.strip() if job.find(\"h2\") else \"N/A\"\n",
    "            company = job.find(\"span\", class_=\"companyName\").text.strip() if job.find(\"span\", class_=\"companyName\") else \"N/A\"\n",
    "            location = job.find(\"div\", class_=\"companyLocation\").text.strip() if job.find(\"div\", class_=\"companyLocation\") else \"N/A\"\n",
    "            salary = job.find(\"div\", class_=\"attribute_snippet\").text.strip() if job.find(\"div\", class_=\"attribute_snippet\") else \"N/A\"\n",
    "            summary = job.find(\"div\", class_=\"job-snippet\").text.strip() if job.find(\"div\", class_=\"job-snippet\") else \"N/A\"\n",
    "            date_posted = job.find(\"span\", class_=\"date\").text.strip() if job.find(\"span\", class_=\"date\") else \"N/A\"\n",
    "\n",
    "            job_list.append({\n",
    "                \"Job Title\": title,\n",
    "                \"Company\": company,\n",
    "                \"Location\": location,\n",
    "                \"Salary\": salary,\n",
    "                \"Job Summary\": summary,\n",
    "                \"Date Posted\": date_posted\n",
    "            })\n",
    "\n",
    "        print(f\"Scraped page {page // 10 + 1}\")\n",
    "        time.sleep(2)  # Pause to avoid being blocked\n",
    "\n",
    "    return pd.DataFrame(job_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step-4: Scrape 5 Pages & Store Data in CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped page 1\n",
      "Scraped page 2\n",
      "Scraped page 3\n",
      "Scraped page 4\n",
      "Scraped page 5\n",
      "Scraping complete. Data saved in 'data/indeed_jobs.csv'.\n"
     ]
    }
   ],
   "source": [
    "df = scrape_indeed_jobs(pages=5)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"../0_Data/indeed_jobs.csv\", index=False)\n",
    "\n",
    "print(\"Scraping complete. Data saved in 'data/indeed_jobs.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code:  403\n",
      "<!DOCTYPE html><html lang=\"en\"><head><meta charset=\"utf-8\"><title>Security Check - Indeed.com</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"><style>:root{color-scheme:light dark;--background-color:#fff;--primary-1000:#0d2d5e;--primary-900:#164081;--primary-800:#2557a7;--primary-700:#3f73d3;--primary-600:#6792f0;--neutral-1000:#2d2d2d;--neutral-900:#424242;--neutral-400:#d4d2d0;--dark-1000:#040606;--link-color:var(--primary-800);--link-color-hover:var(--primary-900);--\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://in.indeed.com/jobs?q=Data+Science&l=India\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "response = requests.get(base_url, headers=headers)\n",
    "print(\"Status Code: \", response.status_code)  # Should return 200 if successful\n",
    "print(response.text[:500])   # Print first 500 characters of HTML response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 jobs on the page.\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "jobs = soup.find_all(\"div\", class_=\"job_seen_beacon\")\n",
    "\n",
    "print(f\"Found {len(jobs)} jobs on the page.\")  # Should not be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job in jobs[:5]:  # Print first 5 job titles\n",
    "    title = job.find(\"h2\").text.strip() if job.find(\"h2\") else \"N/A\"\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 403\n",
      "<!DOCTYPE html><html lang=\"en\"><head><meta charset=\"utf-8\"><title>Security Check - Indeed.com</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"><style>:root{color-scheme:light dark;--background-color:#fff;--primary-1000:#0d2d5e;--primary-900:#164081;--primary-800:#2557a7;--primary-700:#3f73d3;--primary-600:#6792f0;--neutral-1000:#2d2d2d;--neutral-900:#424242;--neutral-400:#d4d2d0;--dark-1000:#040606;--link-color:var(--primary-800);--link-color-hover:var(--primary-900);--\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "\n",
    "# Rotate User-Agent headers to appear like a real browser\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/109.0\"\n",
    "]\n",
    "\n",
    "session = requests.Session()\n",
    "headers = {\n",
    "    \"User-Agent\": random.choice(USER_AGENTS),  # Random User-Agent\n",
    "    \"Referer\": \"https://www.google.com/\",  # Simulate coming from Google\n",
    "}\n",
    "\n",
    "url = \"https://in.indeed.com/jobs?q=Data+Science&l=India\"\n",
    "response = session.get(url, headers=headers)\n",
    "\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(response.text[:500])  # Check if we get actual job listings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Collecting selenium\n",
      "  Using cached selenium-4.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting webdriver-manager\n",
      "  Using cached webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.3.0)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Using cached trio-0.28.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Using cached trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (2025.1.31)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (4.12.2)\n",
      "Collecting websocket-client~=1.8 (from selenium)\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from webdriver-manager) (2.32.3)\n",
      "Collecting python-dotenv (from webdriver-manager)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from webdriver-manager) (24.1)\n",
      "Collecting attrs>=23.2.0 (from trio~=0.17->selenium)\n",
      "  Using cached attrs-25.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (3.10)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Using cached outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sniffio>=1.3.0 (from trio~=0.17->selenium)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting cffi>=1.14 (from trio~=0.17->selenium)\n",
      "  Downloading cffi-1.17.1-cp312-cp312-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Using cached wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3,>=1.26->selenium)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->webdriver-manager) (3.4.1)\n",
      "Collecting pycparser (from cffi>=1.14->trio~=0.17->selenium)\n",
      "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Using cached selenium-4.28.1-py3-none-any.whl (9.5 MB)\n",
      "Using cached webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Using cached trio-0.28.0-py3-none-any.whl (486 kB)\n",
      "Using cached trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Using cached attrs-25.1.0-py3-none-any.whl (63 kB)\n",
      "Downloading cffi-1.17.1-cp312-cp312-win_amd64.whl (181 kB)\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Using cached outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: sortedcontainers, websocket-client, sniffio, python-dotenv, pysocks, pycparser, h11, attrs, wsproto, webdriver-manager, outcome, cffi, trio, trio-websocket, selenium\n",
      "Successfully installed attrs-25.1.0 cffi-1.17.1 h11-0.14.0 outcome-1.3.0.post0 pycparser-2.22 pysocks-1.7.1 python-dotenv-1.0.1 selenium-4.28.1 sniffio-1.3.1 sortedcontainers-2.4.0 trio-0.28.0 trio-websocket-0.11.1 webdriver-manager-4.0.2 websocket-client-1.8.0 wsproto-1.2.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install selenium webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 jobs.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "# Set up Selenium\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # Run in background (remove this to see browser)\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# Open Indeed page\n",
    "driver.get(\"https://in.indeed.com/jobs?q=Data+Science&l=India\")\n",
    "time.sleep(3)  # Wait for page to load\n",
    "\n",
    "# Extract job listings\n",
    "jobs = driver.find_elements(By.CLASS_NAME, \"job_seen_beacon\")\n",
    "print(f\"Found {len(jobs)} jobs.\")\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m title \u001b[38;5;241m=\u001b[39m job\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     22\u001b[0m company \u001b[38;5;241m=\u001b[39m job\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh3\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoblist-comp-name\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m---> 23\u001b[0m location \u001b[38;5;241m=\u001b[39m \u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mul\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop-jd-dtl clearfix\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mli\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     24\u001b[0m skills \u001b[38;5;241m=\u001b[39m job\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrp-skills\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     25\u001b[0m posted_date \u001b[38;5;241m=\u001b[39m job\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msim-posted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of TimesJobs search results for Data Science\n",
    "url = \"https://www.timesjobs.com/candidate/job-search.html?searchType=personalizedSearch&from=submit&txtKeywords=Data+Science&txtLocation=\"\n",
    "\n",
    "# Send a request to fetch the webpage\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find all job postings\n",
    "jobs = soup.find_all(\"li\", class_=\"clearfix job-bx wht-shd-bx\")\n",
    "\n",
    "# Extract job details\n",
    "job_list = []\n",
    "for job in jobs:\n",
    "    title = job.find(\"h2\").text.strip()\n",
    "    company = job.find(\"h3\", class_=\"joblist-comp-name\").text.strip()\n",
    "    location = job.find(\"ul\", class_=\"top-jd-dtl clearfix\").find_all(\"li\")[1].text.strip()\n",
    "    skills = job.find(\"span\", class_=\"srp-skills\").text.strip()\n",
    "    posted_date = job.find(\"span\", class_=\"sim-posted\").text.strip()\n",
    "\n",
    "    job_list.append([title, company, location, skills, posted_date])\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(job_list, columns=[\"Job Title\", \"Company\", \"Location\", \"Skills\", \"Posted Date\"])\n",
    "df.to_csv(\"data/timesjobs_jobs.csv\", index=False)\n",
    "\n",
    "print(\"✅ Scraping completed! Data saved in 'data/timesjobs_jobs.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m     location \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot specified\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 13\u001b[0m skills \u001b[38;5;241m=\u001b[39m \u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msrp-skills\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     14\u001b[0m posted_date \u001b[38;5;241m=\u001b[39m job\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msim-posted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     16\u001b[0m job_list\u001b[38;5;241m.\u001b[39mappend([title, company, location, skills, posted_date])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "for job in jobs:\n",
    "    title = job.find(\"h2\").text.strip()\n",
    "    company = job.find(\"h3\", class_=\"joblist-comp-name\").text.strip()\n",
    "    \n",
    "    # Check if location exists\n",
    "    location_tag = job.find(\"ul\", class_=\"top-jd-dtl clearfix\")\n",
    "    if location_tag:  \n",
    "        location_items = location_tag.find_all(\"li\")  # Get all <li> inside <ul>\n",
    "        location = location_items[1].text.strip() if len(location_items) > 1 else \"Not specified\"\n",
    "    else:\n",
    "        location = \"Not specified\"\n",
    "\n",
    "    skills = job.find(\"span\", class_=\"srp-skills\").text.strip()\n",
    "    posted_date = job.find(\"span\", class_=\"sim-posted\").text.strip()\n",
    "\n",
    "    job_list.append([title, company, location, skills, posted_date])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML page saved as 'timesjobs_page.html'. Open it to inspect the structure.\n"
     ]
    }
   ],
   "source": [
    "# TimesJobs URL for Data Analyst jobs in India\n",
    "url = \"https://www.timesjobs.com/candidate/job-search.html?searchType=personalizedSearch&from=submit&searchTextSrc=as&searchTextText=%22Data+Analyst%22%2CIndia&txtKeywords=%22Data+Analyst%22%2C&cboWorkExp1=2&txtLocation=India\"\n",
    "\n",
    "# Set user-agent to avoid being blocked\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
    "\n",
    "# Send GET request\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Save HTML content to a file\n",
    "with open(\"timesjobs_page.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(response.text)\n",
    "\n",
    "print(\"HTML page saved as 'timesjobs_page.html'. Open it to inspect the structure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scraping completed! Data saved in 'data/timesjobs_jobs.csv'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL for Data Analyst jobs in India\n",
    "url = \"https://www.timesjobs.com/candidate/job-search.html?searchType=personalizedSearch&from=submit&searchTextSrc=as&searchTextText=%22Data+Analyst%22%2CIndia&txtKeywords=%22Data+Analyst%22%2C&cboWorkExp1=2&txtLocation=India\"\n",
    "\n",
    "# Set User-Agent to mimic a real browser\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Send request to TimesJobs\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Parse HTML using BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find all job postings\n",
    "jobs = soup.find_all(\"li\", class_=\"clearfix job-bx wht-shd-bx\")\n",
    "\n",
    "# Extract job details\n",
    "job_list = []\n",
    "for job in jobs:\n",
    "    title = job.find(\"h2\").text.strip() if job.find(\"h2\") else \"N/A\"\n",
    "    company = job.find(\"h3\", class_=\"joblist-comp-name\").text.strip() if job.find(\"h3\", class_=\"joblist-comp-name\") else \"N/A\"\n",
    "    \n",
    "    # Extract multiple locations\n",
    "    location_tags = job.find_all(\"span\", class_=\"srp-zindex location-tru\")\n",
    "\n",
    "    # Join multiple locations into a single string\n",
    "    location = \", \".join([loc.text.strip() for loc in location_tags]) if location_tags else \"Not specified\"\n",
    "\n",
    "\n",
    "    # Extract skills\n",
    "    skills = job.find(\"span\", class_=\"srp-skills\").text.strip() if job.find(\"span\", class_=\"srp-skills\") else \"N/A\"\n",
    "    \n",
    "    # Extract posted date\n",
    "    posted_date = job.find(\"span\", class_=\"sim-posted\").text.strip() if job.find(\"span\", class_=\"sim-posted\") else \"N/A\"\n",
    "\n",
    "    job_list.append([title, company, location, skills, posted_date])\n",
    "\n",
    "# Save extracted data to CSV\n",
    "df = pd.DataFrame(job_list, columns=[\"Job Title\", \"Company\", \"Location\", \"Skills\", \"Posted Date\"])\n",
    "df.to_csv(\"timesjobs_jobs.csv\", index=False)\n",
    "\n",
    "print(\"✅ Scraping completed! Data saved in 'data/timesjobs_jobs.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scraping completed! Data saved in 'data/timesjobs_jobs.csv'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL for Data Analyst jobs in India\n",
    "url = \"https://www.timesjobs.com/candidate/job-search.html?searchType=personalizedSearch&from=submit&searchTextSrc=as&searchTextText=%22Data+Analyst%22%2CIndia&txtKeywords=%22Data+Analyst%22%2C&cboWorkExp1=2&txtLocation=India\"\n",
    "\n",
    "# Set User-Agent to mimic a real browser\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Send request to TimesJobs\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Parse HTML using BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find all job postings\n",
    "jobs = soup.find_all(\"li\", class_=\"clearfix job-bx wht-shd-bx\")\n",
    "\n",
    "# Extract job details\n",
    "job_list = []\n",
    "for job in jobs:\n",
    "    title = job.find(\"h2\").text.strip() if job.find(\"h2\") else \"N/A\"\n",
    "    company = job.find(\"h3\", class_=\"joblist-comp-name\").text.strip() if job.find(\"h3\", class_=\"joblist-comp-name\") else \"N/A\"\n",
    "    \n",
    "    # Extract multiple locations\n",
    "    location_tags = job.find_all(\"span\", class_=\"srp-zindex location-tru\")\n",
    "    location = \", \".join([loc.text.strip() for loc in location_tags]) if location_tags else \"Not specified\"\n",
    "\n",
    "    # Extract skills\n",
    "    skills = job.find(\"span\", class_=\"srp-skills\").text.strip() if job.find(\"span\", class_=\"srp-skills\") else \"N/A\"\n",
    "    \n",
    "    # Extract posted date\n",
    "    posted_date = job.find(\"span\", class_=\"sim-posted\").text.strip() if job.find(\"span\", class_=\"sim-posted\") else \"N/A\"\n",
    "\n",
    "    job_list.append([title, company, location, skills, posted_date])\n",
    "\n",
    "# Save extracted data to CSV\n",
    "df = pd.DataFrame(job_list, columns=[\"Job Title\", \"Company\", \"Location\", \"Skills\", \"Posted Date\"])\n",
    "df.to_csv(\"timesjobs_jobs2.csv\", index=False)\n",
    "\n",
    "print(\"✅ Scraping completed! Data saved in 'data/timesjobs_jobs.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scraping completed! Data saved in 'data/timesjobs_jobs.csv'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL for Data Analyst jobs in India\n",
    "url = \"https://www.timesjobs.com/candidate/job-search.html?searchType=personalizedSearch&from=submit&searchTextSrc=as&searchTextText=%22Data+Analyst%22%2CIndia&txtKeywords=%22Data+Analyst%22%2C&cboWorkExp1=2&txtLocation=India\"\n",
    "\n",
    "# Set User-Agent to mimic a real browser\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Send request to TimesJobs\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Parse HTML using BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find all job postings\n",
    "jobs = soup.find_all(\"li\", class_=\"clearfix job-bx wht-shd-bx\")\n",
    "\n",
    "# Extract job details\n",
    "job_list = []\n",
    "for job in jobs:\n",
    "    title = job.find(\"h2\").text.strip() if job.find(\"h2\") else \"N/A\"\n",
    "    company = job.find(\"h3\", class_=\"joblist-comp-name\").text.strip() if job.find(\"h3\", class_=\"joblist-comp-name\") else \"N/A\"\n",
    "    \n",
    "    # Extract location from the correct tag\n",
    "    location_tag = job.find(\"ul\", class_=\"list-job-dtl clearfix\")\n",
    "    if location_tag:\n",
    "        location_items = location_tag.find_all(\"li\")\n",
    "        location = [li.text.strip() for li in location_items if \"location\" in str(li)]\n",
    "        location = \", \".join(location) if location else \"Not specified\"\n",
    "    else:\n",
    "        location = \"Not specified\"\n",
    "\n",
    "    # Extract skills\n",
    "    skills = job.find(\"span\", class_=\"srp-skills\").text.strip() if job.find(\"span\", class_=\"srp-skills\") else \"N/A\"\n",
    "    \n",
    "    # Extract posted date\n",
    "    posted_date = job.find(\"span\", class_=\"sim-posted\").text.strip() if job.find(\"span\", class_=\"sim-posted\") else \"N/A\"\n",
    "\n",
    "    job_list.append([title, company, location, skills, posted_date])\n",
    "\n",
    "# Save extracted data to CSV\n",
    "df = pd.DataFrame(job_list, columns=[\"Job Title\", \"Company\", \"Location\", \"Skills\", \"Posted Date\"])\n",
    "df.to_csv(\"timesjobs_jobs3.csv\", index=False)\n",
    "\n",
    "print(\"✅ Scraping completed! Data saved in 'data/timesjobs_jobs.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Synthetic job dataset generated and saved as 'data/synthetic_jobs.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "# Define job titles, companies, and skills\n",
    "job_titles = [\"Data Analyst\", \"Senior Data Analyst\", \"Data Scientist\", \"Machine Learning Engineer\"]\n",
    "companies = [\"TCS\", \"Infosys\", \"Google\", \"Amazon\", \"Flipkart\", \"Wipro\", \"Deloitte\"]\n",
    "locations = [\"Bangalore\", \"Mumbai\", \"Delhi\", \"Pune\", \"Hyderabad\", \"Chennai\"]\n",
    "skills = [\"Python, SQL, Power BI\", \"Tableau, Excel, R\", \"Machine Learning, Deep Learning\", \"Data Wrangling, Pandas, NumPy\"]\n",
    "\n",
    "# Generate job listings\n",
    "jobs = []\n",
    "for _ in range(50):  # Generate 50 job listings\n",
    "    job = {\n",
    "        \"Job Title\": random.choice(job_titles),\n",
    "        \"Company\": random.choice(companies),\n",
    "        \"Location\": random.choice(locations) + \", India\",\n",
    "        \"Skills\": random.choice(skills),\n",
    "        \"Experience Required\": f\"{random.randint(1, 10)}+ years\",\n",
    "        \"Salary\": f\"₹{random.randint(6, 20)}L per annum\",\n",
    "        \"Date Posted\": f\"Posted {random.randint(1, 14)} days ago\"\n",
    "    }\n",
    "    jobs.append(job)\n",
    "\n",
    "# Convert to DataFrame & Save\n",
    "df = pd.DataFrame(jobs)\n",
    "df.to_csv(\"synthetic_jobs.csv\", index=False)\n",
    "\n",
    "print(\"✅ Synthetic job dataset generated and saved as 'data/synthetic_jobs.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Collecting faker\n",
      "  Downloading Faker-35.2.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.4 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from faker) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from faker) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
      "Downloading Faker-35.2.0-py3-none-any.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 0.8/1.9 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.8/1.9 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 4.4 MB/s eta 0:00:00\n",
      "Installing collected packages: faker\n",
      "Successfully installed faker-35.2.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install faker"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
